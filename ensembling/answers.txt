Task 4

1. The graph for Bagging shows significant increase in the accuracy scores of traning, validation as well as test data. It is because, in Bagging we train some k number of weak classifiers and train them with sampled data. Thus, the models individually are not too accurate in classifying a data point. We combine the outputs of the k weak classifiers and do a simple voting to find the majority class. When we have less number of classifiers, It is more probable that a majority classifiers misclassify a point, due to which the final output of the point stands mis-classified. Hence, the accuracy score significantly increase for smaller number of classifiers and the rate of increase of accuracy gradually decrease as we move to larger number of classifiers.

The graph for Boosting also shows similar results as that of bagging in the rate of change of increase in accuracy score as we increase the number of classifiers. But after a certain number of classifiers, the accuracy score of validation and test data is observed decreasing. The possible cause of it may be overfitting, because the accuracy score of training data keeps on increasing, which shows that the models tends to learn specific noise in the traning data inorder to give better results which results of making a complex hypothesis instead of a more generalised one.

The accuracy score of Bagging and Boosting for same number of classifiers is similar. Bagging uses a simple voting of the output of K classifiers trained upon K randomly sampled data while Boosting uses a weighted voting of K classifiers trained of a weighted data. The data which were misclassfied before are given more weights so that they get picked up by the next classifiers and classified correctly. Though, Boosting looks more promising on paper, but the data set we have used is significanly small, thus it may not many misclassified points for boosting to show the differnce.

The traning accuracy of Bagging increases and then decreases after a certain number of classifiers. On the other hand, the training accuracy of boosting keeps on increasing making it close to 100. It may be because, the boosting algorithm uses a weighted voting system as well as a weighted sampling to train the classifiers, which makes it a high variance variance model which keeps track of the noises in the training data causing overfitting.

2. The essemble combining perceptrons with weighted majority is better than a single perceptron. Let's assume that the probabilty of misclassifying a point by a single classifier is p. Now if we have more than one classifer, the probablity of misclassifying the same point would be p1 * p2 * ..... which is less than single probabilty. Also, as we are taking the weighted voting, hence we are giving more priority to the best performing classifiers. Hence the performance of the ensemble is expected to be much better than a single perceptron upon a large data set. 